# ğŸ§  Finetuning on 5B Parameters on Custom Dataset â€“ Microsoft Phi | LLM Fine-Tuning

## PEFT Fine-Tuning Project ğŸš€

Welcome to the **PEFT (Parameter-Efficient Fine-Tuning)** project! This repository focuses on **efficiently fine-tuning large language models (LLMs)** using **LoRA** and Hugging Face's **Transformers** library on a custom dataset. The target model is **Microsoft's Phi-2** model with **5 billion parameters**.

---

## ğŸ“Œ Project Goals

- Fine-tune **Microsoft Phi-2 (5B parameters)** on a domain-specific/custom dataset.
- Use **LoRA (Low-Rank Adaptation)** for memory- and compute-efficient fine-tuning.
- Employ **PEFT** and **Hugging Face Transformers** libraries to streamline the process.

---
<!-- README.md -->

<p align="center">
  <img src="https://readme-typing-svg.demolab.com?font=Fira+Code&pause=1000&center=true&vCenter=true&width=500&lines=ğŸš€+Finetuning+on+5B+Parameters;ğŸ’»+Fine-Tuning+Microsoft+Phi-2;âš™ï¸+PEFT+%7C+LoRA+%7C+Transformers" alt="Typing SVG" />
</p>

<p align="center">
  <img src="https://media.giphy.com/media/xT0Gqjs0y0iXc5Yw0s/giphy.gif" width="350" alt="finetuning gif" />
</p>

---

# ğŸ§  **Finetuning on 5B Parameters with Microsoft's Phi-2 â€“ LLM Fine-Tuning**

Welcome to the **PEFT (Parameter-Efficient Fine-Tuning)** project! This repository is designed to efficiently fine-tune the **Microsoft Phi-2** model with **5 billion parameters** on a custom dataset using the power of **LoRA (Low-Rank Adaptation)** and Hugging Faceâ€™s **Transformers** library.

---

## ğŸ“Œ **Project Goals**

- Fine-tune **Microsoft Phi-2 (5B parameters)** on a domain-specific/custom dataset.
- Leverage **LoRA (Low-Rank Adaptation)** for memory- and compute-efficient fine-tuning.
- Use **PEFT** and **Hugging Face Transformers** libraries to streamline and optimize the fine-tuning process.

---

## ğŸ“ **Directory Structure**



## ğŸ“ Directory Structure

```
.
â”œâ”€â”€ dataset/                  # Custom dataset files
â”œâ”€â”€ scripts/                 # Training and evaluation scripts
â”œâ”€â”€ config/                  # LoRA and training configurations
â”œâ”€â”€ outputs/                 # Model checkpoints and logs
â”œâ”€â”€ README.md                # This file
```

---

## âš™ï¸ Tech Stack

- ğŸ— **Model**: Microsoft Phi-2 (5B)
- ğŸ§© **Fine-Tuning Strategy**: LoRA (via `peft` library)
- ğŸ§  **Frameworks**: Hugging Face Transformers, Datasets, Accelerate
- ğŸš€ **Training**: Optimized for GPU environments (A100 recommended)

---

## ğŸ“ Installation

```bash
git clone https://github.com/yourusername/Finetuning-on-5b-parameters-on-Custom_data_set-Microsoft-phi-.git
cd Finetuning-on-5b-parameters-on-Custom_data_set-Microsoft-phi-
pip install -r requirements.txt
```

---

## ğŸƒâ€â™‚ï¸ Quick Start

```bash
python scripts/train_lora.py \
  --model_name microsoft/phi-2 \
  --dataset_path ./dataset/custom_data.json \
  --output_dir ./outputs \
  --peft_config ./config/lora_config.json
```

---

## ğŸ§ª Evaluation

Use the evaluation script to test your fine-tuned model:

```bash
python scripts/eval_model.py --model_dir ./outputs
```

---

## ğŸ“Š Results

| Metric       | Value     |
|--------------|-----------|
| Perplexity   | XX.XX     |
| F1 Score     | XX.XX     |
| Accuracy     | XX.XX%    |

*Note: Results depend on dataset quality and training duration.*

---

## ğŸ§  Acknowledgements

- Microsoft for the **Phi-2** model.
- Hugging Face for the **Transformers** and **PEFT** libraries.
- LoRA by Microsoft Research.

---

## ğŸ“„ License

This project is licensed under the MIT License.
.

